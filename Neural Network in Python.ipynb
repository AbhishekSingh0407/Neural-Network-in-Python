{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41708a3-1562-4529-b5a8-0970242a15f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n",
      "4.8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"modelling 1 neuron with 4 input\"\"\"\n",
    "\n",
    "inputs = [1,2,3,2.5]\n",
    "weights = [0.2,0.8,-0.5,1]\n",
    "bias =2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "print(output)\n",
    "\n",
    "output=0\n",
    "for i in range(0,len(inputs)):\n",
    "    output = output + inputs[i]*weights[i]\n",
    "\n",
    "print(output +bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f5d5ff-bc99-472e-b29f-11435e82a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"modelling 3 neurons with 4 inputs\n",
    "\n",
    "showcasing the output layer of NN\"\"\"\n",
    "\n",
    "inputs = [1,2,3,2.5]\n",
    "weights1 = [0.2,0.8,-0.5,1]\n",
    "weights2 = [0.5,-0.91,0.26,-0.5]\n",
    "weights3 = [-0.26,-0.27,0.17,0.87]\n",
    "\n",
    "bias1 =2\n",
    "bias2 =3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = [\n",
    "    inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "    inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "    inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3\n",
    "]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f34e03f-f8ec-4f43-8b3c-e225df28bb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"with loops\"\"\"\n",
    "inputs = [1,2,3,2.5]\n",
    "\n",
    "weights = [ [0.2,0.8,-0.5,1],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87] ]\n",
    "\n",
    "biases = [2,3,0.5] \n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "# result = zip(weights, biases)\n",
    "# print(list(result))\n",
    "\n",
    "for neuron_weight,neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    for n_input,weight in zip(inputs, neuron_weight):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2887ed70-96bd-4fab-8914-7868333b9919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "# dot product\n",
    "inputs = [1,2,3,2.5]\n",
    "weights = [0.2,0.8,-0.5,1]\n",
    "bias = 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "output = np.dot(inputs,weights) + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8cb9220-4321-43fc-99d6-992a3e50236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1,2,3,2.5]\n",
    "\n",
    "weights = [ [0.2,0.8,-0.5,1],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87] ]\n",
    "\n",
    "biases = [2,3,0.5] \n",
    "\n",
    "# output = np.dot(inputs,weights) + biases # wont work --> ValueError: shapes (4,) and (3,4) not aligned: 4 (dim 0) != 3 (dim 0)\n",
    "output = np.dot(weights,inputs) + biases\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9210a02-d4c9-4c57-844b-6f523813fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n",
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\"\"\"showcasing Basic Relu\"\"\"\n",
    "\n",
    "inputs = [0,2,-1,3.3, -2.7,1.1,2.2,-100]\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    if i>0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "\n",
    "print(output)\n",
    "\n",
    "output = []\n",
    "for i in inputs:\n",
    "    output.append(max(0,i))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52f931f-55da-4781-853e-057e8b2313a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "layer1 = Layer_Dense(4,5)\n",
    "layer2 = Layer_Dense(5,2)\n",
    "\n",
    "layer1.forward(X)\n",
    "#print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2adc3152-a168-4506-bb2e-f0c91a398347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1.output [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-8.35815910e-04 -7.90404272e-04 -1.33452227e-03  4.65504505e-04\n",
      "   4.56846210e-05]\n",
      " [-2.39994470e-03  5.93469958e-05 -2.24808278e-03  2.03573116e-04\n",
      "   6.10024377e-04]\n",
      " ...\n",
      " [ 1.13291524e-01 -1.89262271e-01 -2.06855070e-02  8.11079666e-02\n",
      "  -6.71350807e-02]\n",
      " [ 1.34588361e-01 -1.43197834e-01  3.09493970e-02  5.66337556e-02\n",
      "  -6.29687458e-02]\n",
      " [ 1.07817926e-01 -2.00809643e-01 -3.37579325e-02  8.72561932e-02\n",
      "  -6.81458861e-02]]\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
      "  4.56846210e-05]\n",
      " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
      "  6.10024377e-04]\n",
      " ...\n",
      " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Relu Activation \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#pip install nnfs\n",
    "#nnfs\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# np.random.seed(0)\n",
    "nnfs.init()     #sets defaults data type for using Numpy, as np.dot uses some other data type which creates complication further.\n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "X,y = spiral_data(100,3)\n",
    "\n",
    "# print(np.random.randn(2,4))\n",
    "\n",
    "\n",
    "class Layer_Dense():\n",
    "    def __init__(self, n_input, n_neurons) -> None:\n",
    "        self.weights = 0.10 * np.random.randn(n_input, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "\n",
    "class Activation_Relu():\n",
    "    def forward(self, inputs) -> None:\n",
    "        self.output =  np.maximum(0,inputs)\n",
    "\n",
    "layer1 = Layer_Dense(2,5)\n",
    "activation1 = Activation_Relu()\n",
    "# layer2 = Layer_Dense(5,2)\n",
    "\n",
    "layer1.forward(X)\n",
    "print(\"Layer1.output\",layer1.output)\n",
    "activation1.forward(layer1.output)  # this should make all the -ve values in Layer1.output, zero\n",
    "print(activation1.output)\n",
    "\n",
    "# layer2.forward(layer1.output)\n",
    "# print(\"Layer2.output\",layer2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f7e66d-b129-4644-bb02-3c70b1a42888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Basic python softmax activation\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "layer_outputs = [4.8,1.21,2.385]\n",
    "E = math.e\n",
    "\n",
    "exp_values =[]\n",
    "\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(math.pow(E,output))\n",
    "\n",
    "#Expontiated values\n",
    "print(exp_values)\n",
    "\n",
    "# Normalization\n",
    "norm_base = sum(exp_values)\n",
    "norm_values =[]\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value/norm_base)\n",
    "\n",
    "print(norm_values)\n",
    "print(sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef650f0f-ff4c-458b-a927-a7abe28965b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041752   3.35348465  10.85906266]\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Basic Numpy softmax activation\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "layer_outputs = [4.8,1.21,2.385]\n",
    "\n",
    "exp_values =[]\n",
    "exp_values = np.exp(layer_outputs)\n",
    "\n",
    "#Expontiated values\n",
    "print(exp_values)\n",
    "\n",
    "# Normalization\n",
    "norm_values =[]\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "\n",
    "print(norm_values)\n",
    "print(np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d410ff1b-27ec-4ec4-b599-78c97db38215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.172\n",
      "18.172\n",
      "[15.11   0.451  2.611]\n",
      "[8.395 7.29  2.487]\n",
      "[[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "arr = [[4.8,1.21,2.385],\n",
    "       [8.9,-1.81,0.2],\n",
    "       [1.41,1.051,0.026]]\n",
    "\n",
    "print(np.sum(arr))\n",
    "print(np.sum(arr, axis=None))\n",
    "\n",
    "print(np.sum(arr, axis=0))   # Column wise sum\n",
    "print(np.sum(arr, axis=1))    # Row  wise sum\n",
    "\n",
    "# If keepdims is set to True, the axes which are reduced are left in the result as dimensions with size one.\n",
    "# With this option, the result will broadcast correctly against the input array.\n",
    "print(np.sum(arr, axis=1, keepdims=True))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f47f28-6f88-44c4-8e92-e4bfc53262d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [[4.8,1.21,2.385],\n",
    "       [8.9,-1.81,0.2],\n",
    "       [1.41,1.051,0.026]]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b41e46a-721d-45e3-bc60-4ae7bd3de445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.62510862e-02 4.48502847e-04 1.45231633e-03]\n",
      " [9.80595215e-01 2.18874853e-05 1.63353249e-04]\n",
      " [5.47802614e-04 3.82571295e-04 1.37265228e-04]]\n",
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_outputs = [[4.8,1.21,2.385],\n",
    "       [8.9,-1.81,0.2],\n",
    "       [1.41,1.051,0.026]]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "\n",
    " # does not give true normalized value, as it is taking sum of the entire outputs rather than 1 output \n",
    "norm_values = exp_values / np.sum(exp_values) \n",
    "print(norm_values)\n",
    "\n",
    "norm_values = exp_values / np.sum(exp_values,axis=1, keepdims=True)\n",
    "print(norm_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef79d150-9e87-4ac1-9873-c83d786cd9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.3333183  0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"showcasing  Relu and softmax activations object\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "np.random.seed(0)\n",
    "nnfs.init()     #sets defaults data type for using Numpy, as np.dot uses some other data type which creates complication further.\n",
    "\n",
    "X,y = spiral_data(samples =100,classes= 3)\n",
    "\n",
    "class Layer_Dense():\n",
    "    def __init__(self, n_input, n_neurons) -> None:\n",
    "        self.weights = 0.10 * np.random.randn(n_input, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "\n",
    "class Activation_Relu():\n",
    "    def forward(self, inputs) -> None:\n",
    "        self.output =  np.maximum(0,inputs)\n",
    "\n",
    "class Activation_Softmax():\n",
    "    def forward(self,inputs):\n",
    "        exp_values = np.exp(inputs-np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities    \n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_Relu()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)  # this should make all the -ve values in dense.output, zero\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])  #5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0105d28f-50cf-41eb-be50-52ef9c1c0567",
   "metadata": {},
   "source": [
    "The distirbution of prediction is close to perfect 1/3rd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee70c8d-63d2-4aa2-9035-1d7c743668bb",
   "metadata": {},
   "source": [
    "What is logarithm ?\n",
    "\n",
    "--> solving for x  \n",
    "e**x = b , wher e is euler's number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15dc852d-92d9-4c4b-b798-470fb49d24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n"
     ]
    }
   ],
   "source": [
    "b=5.2\n",
    "print(np.log(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a124516-e4c5-42d6-96a6-c5f2f9c26d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "print(math.e**1.6486586255873816)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba76583-15f5-47f1-aaf7-2dd0907f716a",
   "metadata": {},
   "source": [
    "Calculating loss with catgorical cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70afbace-1626-4aac-8efe-d09641383150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "softmax_output = [0.7,0.1,0.2]    #sum equals 1\n",
    "# target_class = 0   # vector is hot at index 0\n",
    "target_output =[1,0,0]\n",
    "\n",
    "loss = -(target_output[0]*math.log(softmax_output[0]) +\n",
    "            target_output[1]*math.log(softmax_output[1]) +\n",
    "            target_output[2]*math.log(softmax_output[2]))\n",
    "                                      \n",
    "print(loss)                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84472e7f-2114-4d5e-9338-266330c9ab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "loss = -(target_output[0]*math.log(softmax_output[0]))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6715c8a9-84e8-41a8-8812-882a4c653369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "print(-math.log(softmax_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96cbc6f9-3771-410d-ba8e-79081fefeb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "print(-math.log(0.7))\n",
    "print(-math.log(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8ed32-9698-417f-b860-9b908f42b53e",
   "metadata": {},
   "source": [
    "in [36] we can see as the confidence was higher in correct class the loss is lower.  \n",
    "And where the condifence in the correct class gets lower the loss( measurement of error) gets higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1f316-c5d8-4df3-bf38-f44ca05c4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Batch processing\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7,0.1,0.2],\n",
    "                            [0.1,0.5,0.4],\n",
    "                            [0.02,0.9,0.08]])\n",
    "class_targets =[0,1,1]\n",
    "\n",
    "print(softmax_outputs[[0,1,2],class_targets])\n",
    "print(softmax_outputs[[0,1,2],0])\n",
    "\n",
    "print(np.mean(-np.log(softmax_outputs[[0,1,2],class_targets])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cb39996-e939-47a8-beea-76e336fa817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999\n"
     ]
    }
   ],
   "source": [
    "print(1-1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17fe24-3207-44c5-888f-1f4656c47017",
   "metadata": {},
   "source": [
    "## Incorporate Categorical loss entropy in the running example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f681c9-cc1a-4205-b26c-8682b3a9f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.098445\n",
      "acc:  0.34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "np.random.seed(0)\n",
    "nnfs.init()     #sets defaults data type for using Numpy, as np.dot uses some other data type which creates complication further.\n",
    "\n",
    "X,y = spiral_data(samples =100,classes= 3)\n",
    "\n",
    "class Layer_Dense():\n",
    "    def __init__(self, n_input, n_neurons) -> None:\n",
    "        self.weights = 0.10 * np.random.randn(n_input, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "\n",
    "class Activation_Relu():\n",
    "    def forward(self, inputs) -> None:\n",
    "        self.output =  np.maximum(0,inputs)\n",
    "\n",
    "class Activation_Softmax():\n",
    "    def forward(self,inputs):\n",
    "        exp_values = np.exp(inputs-np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities    \n",
    "\n",
    "class Loss():\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples =len(y_pred)\n",
    "        y_pred_clipped =np.clip(y_pred,1e-7, 1-1e-7)  # to avoid memory overflow due to exponentiation\n",
    "        \n",
    "        if len(y_true.shape)==1:   #checks for scalar values\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape) ==2:  #1 hot encoded vector being passed\n",
    "            correct_confidences= np.sum(y_pred_clipped*y_true,axis=1)\n",
    "        \n",
    "        negative_log_liklihoods = -np.log(correct_confidences)\n",
    "        return negative_log_liklihoods\n",
    "    \n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_Relu()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)  # this should make all the -ve values in dense.output, zero\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# print(activation2.output[:5])  #5 rows\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_function.calculate(activation2.output,y)\n",
    "\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Accuracy calculation\n",
    "predictions =np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(\"acc: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "185dba7e-177a-4aa3-a5cc-6d01e0cecf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import vertical_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "X,y = vertical_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "526f6cfa-b9b9-4d8f-b188-a833f0ae1c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(300, 2)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "# plt.scatter(X[:,0], X[:,1], c=y, s=40, cmap='brg')\n",
    "# plt.show()\n",
    "\n",
    "print(np.ndim(X))\n",
    "print(np.shape(X))\n",
    "print(np.shape(np.zeros((2, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bf74367-0c23-418d-8b33-b43e2f4e010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New set of weights found, iteration: 0 loss: 1.1528968 acc: 0.31333333333333335\n",
      "New set of weights found, iteration: 1 loss: 1.1461446 acc: 0.3333333333333333\n",
      "New set of weights found, iteration: 2 loss: 1.1053249 acc: 0.38333333333333336\n",
      "New set of weights found, iteration: 21 loss: 1.1035887 acc: 0.3333333333333333\n",
      "New set of weights found, iteration: 25 loss: 1.100974 acc: 0.3333333333333333\n",
      "New set of weights found, iteration: 36 loss: 1.052621 acc: 0.6633333333333333\n",
      "New set of weights found, iteration: 96 loss: 1.0261556 acc: 0.37\n",
      "New set of weights found, iteration: 373 loss: 1.0092345 acc: 0.62\n",
      "New set of weights found, iteration: 395 loss: 0.97559196 acc: 0.48333333333333334\n",
      "New set of weights found, iteration: 1512 loss: 0.97331667 acc: 0.45666666666666667\n",
      "New set of weights found, iteration: 9281 loss: 0.959499 acc: 0.55\n",
      "New set of weights found, iteration: 10815 loss: 0.9367594 acc: 0.66\n",
      "New set of weights found, iteration: 11888 loss: 0.9353194 acc: 0.61\n",
      "New set of weights found, iteration: 15697 loss: 0.9342942 acc: 0.71\n",
      "New set of weights found, iteration: 22251 loss: 0.93054056 acc: 0.5766666666666667\n",
      "New set of weights found, iteration: 29427 loss: 0.82870626 acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Object instantiation\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "lowest_loss = 999999\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(100000):   #epochs\n",
    "    dense1.weights = 0.5* np.random.randn(2,3)\n",
    "    dense1.biases =  0.5* np.random.randn(1,3)\n",
    "    dense2.weights = 0.5* np.random.randn(3,3)\n",
    "    dense2.biases = 0.5* np.random.randn(1,3)\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    loss = loss_function.calculate(activation2.output,y)\n",
    "\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration, 'loss:', loss, 'acc:',accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4fbea4-858f-4af4-a3fc-2f40f9c21476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
